import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime
import openai   # pip install openai

# --- CONFIGURA√á√ïES ---
OPENAI_API_KEY = "SEU_OPENAI_API_KEY"  # ou use st.secrets
openai.api_key = OPENAI_API_KEY
MODEL = "gpt-4o-mini"  # ajuste conforme disponibilidade

# --- UI ---
st.set_page_config(page_title="Agente de Suporte - Vazamentos", layout="wide")
st.title("ü§ñ Agente de Suporte ‚Äî Cen√°rio de Vazamentos (SIMULADO)")
st.write("Este agente **n√£o** recebe nem grava dados reais. Aqui voc√™ pode simular ocorr√™ncias e conversar com o assistente especializado em vazamentos.")

# sess√£o para armazenar simula√ß√£o em mem√≥ria
if "sim_df" not in st.session_state:
    st.session_state.sim_df = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

col1, col2 = st.columns([2, 1])

with col2:
    st.markdown("### Gerar gr√°fico simulado")
    window_days = st.number_input("Janela (horas)", min_value=1, max_value=168, value=24)
    intensity = st.slider("Intensidade m√©dia (ocorr√™ncias/h)", 0.1, 20.0, 2.0)
    burstiness = st.slider("Risco de picos (0=suave, 1=explosivo)", 0.0, 1.0, 0.3)
    noise = st.slider("Ru√≠do (%)", 0.0, 1.0, 0.1)
    generate = st.button("‚ñ∂Ô∏è Gerar gr√°fico de ocorr√™ncias simuladas")

    if generate:
        # gerar tempos (hora-a-hora) para a janela
        now = datetime.datetime.now()
        periods = int(window_days)  # horas
        times = [now - datetime.timedelta(hours=i) for i in range(periods)][::-1]

        # Simula√ß√£o: processo Poisson com picos aleat√≥rios (burst)
        base_rate = intensity
        rates = np.random.normal(loc=base_rate, scale=base_rate * noise, size=periods)
        # adicionar picos aleat√≥rios
        num_peaks = max(1, int(burstiness * 5))
        for _ in range(num_peaks):
            peak_pos = np.random.randint(0, periods)
            peak_height = base_rate * (5 + 10 * burstiness) * np.random.rand()
            # espalha o pico em uma janela curta
            spread = max(1, int(3 * (1 + burstiness*4)))
            for s in range(-spread, spread+1):
                idx = peak_pos + s
                if 0 <= idx < periods:
                    rates[idx] += peak_height * np.exp(-abs(s)/2)

        # garantir n√£o-negatividade
        rates = np.clip(rates, 0.0, None)

        # gerar contagens (Poisson)
        counts = np.random.poisson(rates)
        df = pd.DataFrame({"timestamp": times, "ocorrencias": counts})
        st.session_state.sim_df = df

        # resumo para o agente usar
        summary = {
            "media_por_hora": float(df.ocorrencias.mean()),
            "maximo": int(df.ocorrencias.max()),
            "horario_maximo": df.loc[df.ocorrencias.idxmax(), "timestamp"].strftime("%Y-%m-%d %H:%M"),
            "total_ocorrencias": int(df.ocorrencias.sum()),
            "janela_horas": periods
        }
        st.session_state.sim_summary = summary
        st.success("Simula√ß√£o gerada. Use o chat para pedir an√°lises baseadas na simula√ß√£o.")

with col1:
    # gr√°fico (se existir)
    if st.session_state.sim_df is not None:
        st.markdown("### Gr√°fico de ocorr√™ncias simuladas")
        fig, ax = plt.subplots(figsize=(10, 3))
        ax.plot(st.session_state.sim_df["timestamp"], st.session_state.sim_df["ocorrencias"])
        ax.set_xlabel("Hora")
        ax.set_ylabel("Ocorr√™ncias")
        ax.grid(True)
        st.pyplot(fig)
    else:
        st.info("Nenhuma simula√ß√£o gerada ainda. Abra o painel direito e clique em 'Gerar gr√°fico de ocorr√™ncias simuladas'.")

st.markdown("---")
st.markdown("### Conversa com o Agente (contexto: vazamentos)")

# Prepare system prompt fixo ‚Äî agente j√° preparado para vazamentos
system_prompt = """
Voc√™ √© EVA, assistente virtual especializado em suporte a incidentes de vazamento em ambientes industriais.
Seu papel: orientar o usu√°rio sobre diagn√≥stico inicial, medidas de conten√ß√£o imediata, sinais que indicam falsos-positivos e passos para acionamento de equipe t√©cnica.
Voc√™ N√ÉO tem dados reais do sensor; voc√™ pode usar a simula√ß√£o gerada pelo usu√°rio (caso exista) para fundamentar respostas.
Se houver um resumo de simula√ß√£o fornecido, mencione-o de forma transparente (ex.: "Na simula√ß√£o x..."). Forne√ßa instru√ß√µes pr√°ticas, passo-a-passo e sugest√µes de prioriza√ß√£o.
Sempre pe√ßa ao usu√°rio para confirmar condi√ß√µes de seguran√ßa e acionar emerg√™ncia se houver risco √† integridade humana.
"""

# display conversation
for entry in st.session_state.chat_history:
    role, text = entry
    if role == "user":
        st.markdown(f"**Voc√™:** {text}")
    else:
        st.markdown(f"**EVA:** {text}")

# input
user_msg = st.text_input("Digite sua pergunta para o agente (ex.: 'O que fazer agora?')")

if st.button("Enviar") and user_msg:
    # compor mensagem com contexto simulado (se houver)
    context_text = ""
    if st.session_state.get("sim_summary"):
        s = st.session_state.sim_summary
        context_text = (
            f"Resumo da simula√ß√£o:\n"
            f"- janela (horas): {s['janela_horas']}\n"
            f"- m√©dia por hora: {s['media_por_hora']:.2f}\n"
            f"- total ocorr√™ncias: {s['total_ocorrencias']}\n"
            f"- pico m√°ximo: {s['maximo']} √†s {s['horario_maximo']}\n"
        )

    # construir prompt para API
    messages = [
        {"role": "system", "content": system_prompt},
    ]
    if context_text:
        messages.append({"role": "system", "content": f"Contexto adicional (simula√ß√£o):\n{context_text}"})
    # adicionar hist√≥rico local (opcional)
    # enviar pergunta atual
    messages.append({"role": "user", "content": user_msg})

    # chamada ao LLM
    try:
        resp = openai.ChatCompletion.create(
            model=MODEL,
            messages=messages,
            max_tokens=400
        )
        answer = resp.choices[0].message.content.strip()
    except Exception as e:
        answer = f"Erro ao chamar LLM: {e}"

    # guardar no hist√≥rico da sess√£o e mostrar
    st.session_state.chat_history.append(("user", user_msg))
    st.session_state.chat_history.append(("assistant", answer))
    st.experimental_rerun()
